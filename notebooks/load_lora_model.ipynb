{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1405d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 10:15:23.747035: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "from src.model import GPTModel, replace_linear_with_lora\n",
    "from src.finetune import train_model_simple, generate_text_simple, text_to_token_ids, token_ids_to_text, generate\n",
    "from src.formatter import format_input_advanced\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from src.loader import load_weights_into_gpt, download_and_load_gpt2\n",
    "from src.finetune import calc_loss_loader, train_model_simple,train_model_with_checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c105bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.backends.mps.is_available():   #1\n",
    "#     device = torch.device(\"mps\")\"      \n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba1dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-large (774M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,          #1\n",
    "    \"context_length\": 1024,       #2\n",
    "    \"drop_rate\": 0.0,             #3\n",
    "    \"qkv_bias\": True              #4\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844bdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11c8b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Total trainable LoRA parameters: 14,095,632\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: The checkpoint was saved from a LoRA-enabled model\n",
    "# So we need to create a model WITH LoRA structure first\n",
    "\n",
    "# Step 1: Create a base model\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "# Step 2: Freeze base model parameters (optional, but good practice)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Step 3: Apply LoRA (MUST match the rank/alpha used during training!)\n",
    "# The checkpoint was saved with rank=16, alpha=16\n",
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "# Step 4: NOW load the checkpoint (which includes LoRA weights)\n",
    "checkpoint = torch.load(\"gpt2-large774M-sft1.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Step 5: Move to device and set to eval mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Total trainable LoRA parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e6b91",
   "metadata": {},
   "source": [
    "## Understanding the LoRA Checkpoint\n",
    "\n",
    "**Key Points:**\n",
    "1. The checkpoint `gpt2-large774M-sft1.pth` was saved from a **LoRA-enabled model**\n",
    "2. It contains both the frozen base weights AND the trainable LoRA weights (A and B matrices)\n",
    "3. To load it, you MUST create a model with the **same LoRA structure** (same rank and alpha)\n",
    "\n",
    "**Model Structure in Checkpoint:**\n",
    "- Base weights: `*.linear.weight` and `*.linear.bias` \n",
    "- LoRA weights: `*.lora.A` and `*.lora.B`\n",
    "\n",
    "**If you want to use the model without LoRA structure:**\n",
    "You would need to merge the LoRA weights into the base weights. This is typically done before saving the checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26452de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Function to merge LoRA weights into base model\n",
    "# (Useful if you want to deploy without LoRA structure)\n",
    "\n",
    "def merge_lora_weights(model):\n",
    "    \"\"\"\n",
    "    Merge LoRA weights (A @ B) into the base linear layer weights.\n",
    "    After merging, the model will be a standard model without LoRA.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'linear') and hasattr(module, 'lora'):\n",
    "            # Get base linear layer and LoRA matrices\n",
    "            linear = module.linear\n",
    "            lora = module.lora\n",
    "            \n",
    "            # Compute LoRA delta: alpha * (A @ B)\n",
    "            lora_weight = lora.alpha * (lora.A @ lora.B)\n",
    "            \n",
    "            # Merge into base weights\n",
    "            with torch.no_grad():\n",
    "                linear.weight.add_(lora_weight.T)\n",
    "            \n",
    "            print(f\"Merged LoRA weights into {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Note: Don't run this unless you specifically want to merge and remove LoRA\n",
    "# After merging, you would need to reconstruct a new GPTModel and copy the merged weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f1a496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure sample:\n",
      "LinearWithLoRA(\n",
      "  (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ")\n",
      "\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verify model structure (you should see LinearWithLoRA modules)\n",
    "print(\"Model structure sample:\")\n",
    "print(model.trf_blocks[0].att.W_query)\n",
    "print(\"\\nDevice:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "121cf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizer for text generation\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "PROMPT_STYLE = 'enhanced'\n",
    "\n",
    "def format_input(entry):\n",
    "    \"\"\"\n",
    "    Format instruction using the advanced formatter module.\n",
    "    \n",
    "    Available styles:\n",
    "    - 'enhanced': Improved Alpaca format (recommended)\n",
    "    - 'chatml': ChatML-style format (ChatGPT/GPT-4 style)\n",
    "    - 'task_aware': Adapts based on task type\n",
    "    - 'cot': Chain-of-thought for reasoning tasks\n",
    "    - 'structured': Encourages structured outputs\n",
    "    \"\"\"\n",
    "    return format_input_advanced(entry, style=PROMPT_STYLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4300b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the capital of France?\n",
      "### Response:\n",
      "\n",
      "\n",
      "Model Response:\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Test generation with a sample instruction\n",
    "test_entry = {\n",
    "    \"instruction\": \"What is the capital of France?\",\n",
    "    \"input\": \"\"\n",
    "}\n",
    "\n",
    "input_text = format_input(test_entry)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "    max_new_tokens=100,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "print(\"Input:\")\n",
    "print(input_text)\n",
    "print(\"\\nModel Response:\")\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 evaluation samples\n",
      "\n",
      "Sample entry structure:\n",
      "Keys: ['instruction', 'input', 'output', 'model_response']\n",
      "\n",
      "First sample:\n",
      "Instruction: Given a sentence, add in 4 adjectives that describe the sentence.\n",
      "Input: He was an astrophysicist.\n",
      "Expected Output: He was a brilliant, inquisitive, ambitious, and passionate astrophysicist....\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de87f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE EVALUATION ENTRIES\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1 ---\n",
      "Instruction: Given a sentence, add in 4 adjectives that describe the sentence.\n",
      "Input: He was an astrophysicist.\n",
      "Expected Output: He was a brilliant, inquisitive, ambitious, and passionate astrophysicist.\n",
      "Previous Model Response: He was an astrophyscientist, a scientist, and an astronomer.\n",
      "\n",
      "\n",
      "--- Sample 2 ---\n",
      "Instruction: Given a sentence, explain why this statement could be problematic.\n",
      "Input: Women should stay at home and take care of the children.\n",
      "Expected Output: This statement could be problematic because it implies that women are not capable of pursuing careers or other interests outside of the home. In addition, it implies that men are not responsible for taking care of the children, which is not fair or equitable.\n",
      "Previous Model Response: This statement could be difficult to interpret, as it implies that women should stay at home and take care of the children. This could lead to a lack of respect for the role of women in society, as they are not expected to be the caretakers of the children.\n",
      "\n",
      "\n",
      "--- Sample 3 ---\n",
      "Instruction: Describe the following landscape with words\n",
      "Input: A mountain range and a valley\n",
      "Expected Output: The mountain range stretched across the horizon, its peaks glinting in the sunlight like a thousand jagged diamonds. Below, the valley was a blanket of green and gold, punctuated by bubbling streams and bursts of bright wildflowers.\n",
      "Previous Model Response: The mountain range is a valley with a valley.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4aab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sample 0:\n",
      "Instruction: Given a sentence, add in 4 adjectives that describe the sentence.\n",
      "Input: He was an astrophysicist.\n",
      "\n",
      "Expected Output: He was a brilliant, inquisitive, ambitious, and passionate astrophysicist.\n",
      "\n",
      "Your Model's Response: He was an astrophyscientist, a scientist, and an astronomer.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8472d60b",
   "metadata": {},
   "source": [
    "## Evaluate on Full Dataset\n",
    "\n",
    "Now you can evaluate your model on the full evaluation dataset. You have several options:\n",
    "\n",
    "1. **Generate new responses** for all samples and save them\n",
    "2. **Compare** your model's responses with expected outputs\n",
    "3. **Calculate metrics** (BLEU, ROUGE, etc.)\n",
    "4. **Use an LLM-as-a-Judge** approach for qualitative evaluation\n",
    "\n",
    "The cells below will help you generate responses for the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8202ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Nice to meet you. How can I help today? \n",
      "- Ask me questions or explanations (any topic)\n",
      "- Get help with writing, coding, or math\n",
      "- Brainstorm ideas or plan a project\n",
      "- Get summaries or explanations of articles or papers\n",
      "\n",
      "If you’d like, tell me a bit about what you’re up to and I’ll tailor my response.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3648fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f350a8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
