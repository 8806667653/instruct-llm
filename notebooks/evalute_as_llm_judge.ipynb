{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5b7379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 evaluation samples\n",
      "\n",
      "Sample entry structure:\n",
      "Keys: ['instruction', 'input', 'output', 'model_response']\n",
      "\n",
      "First sample:\n",
      "Instruction: Given a sentence, add in 4 adjectives that describe the sentence.\n",
      "Input: He was an astrophysicist.\n",
      "Expected Output: He was a brilliant, inquisitive, ambitious, and passionate astrophysicist....\n"
     ]
    }
   ],
   "source": [
    "# Load the evaluation dataset\n",
    "import json\n",
    "\n",
    "# Load the evaluation data\n",
    "with open(\"lora-best_model_response.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(eval_data)} evaluation samples\")\n",
    "print(f\"\\nSample entry structure:\")\n",
    "print(f\"Keys: {list(eval_data[0].keys())}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"Instruction: {eval_data[0]['instruction']}\")\n",
    "print(f\"Input: {eval_data[0]['input']}\")\n",
    "print(f\"Expected Output: {eval_data[0]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3298f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE EVALUATION ENTRIES\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1 ---\n",
      "Instruction: Given a sentence, add in 4 adjectives that describe the sentence.\n",
      "Input: He was an astrophysicist.\n",
      "Expected Output: He was a brilliant, inquisitive, ambitious, and passionate astrophysicist.\n",
      "Previous Model Response: He was an astrophysicist with a passion for science.\n",
      "\n",
      "\n",
      "--- Sample 2 ---\n",
      "Instruction: Given a sentence, explain why this statement could be problematic.\n",
      "Input: Women should stay at home and take care of the children.\n",
      "Expected Output: This statement could be problematic because it implies that women are not capable of pursuing careers or other interests outside of the home. In addition, it implies that men are not responsible for taking care of the children, which is not fair or equitable.\n",
      "Previous Model Response: This statement could be problematic because it implies that women should be at home and take care of the children, which is a sexist and harmful statement.\n",
      "\n",
      "\n",
      "--- Sample 3 ---\n",
      "Instruction: Describe the following landscape with words\n",
      "Input: A mountain range and a valley\n",
      "Expected Output: The mountain range stretched across the horizon, its peaks glinting in the sunlight like a thousand jagged diamonds. Below, the valley was a blanket of green and gold, punctuated by bubbling streams and bursts of bright wildflowers.\n",
      "Previous Model Response: The landscape is a mountain range and a valley. The mountain range is a tall, steep mountain with a wide valley below. The valley is a wide valley with a small mountain in the center.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect a few samples from the evaluation dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE EVALUATION ENTRIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, entry in enumerate(eval_data[:3]):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Instruction: {entry['instruction']}\")\n",
    "    if entry['input']:\n",
    "        print(f\"Input: {entry['input']}\")\n",
    "    print(f\"Expected Output: {entry['output']}\")\n",
    "    if 'model_response' in entry:\n",
    "        print(f\"Previous Model Response: {entry['model_response']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb8565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://lightning.ai/api/v1/\",\n",
    "    api_key=\"XXXXXXXX\",\n",
    ")\n",
    "\n",
    "print(\"OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187962b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Judge Prompt:\n",
      "================================================================================\n",
      "You are an expert evaluator assessing the quality of AI model responses. \n",
      "Your task is to determine if the model's response correctly addresses the given instruction.\n",
      "\n",
      "**Instruction:** Given a sentence, add in 4 adjectives that describe the sentence.\n",
      "\n",
      "**Input:** He was an astrophysicist.\n",
      "\n",
      "**Expected Response:** He was a brilliant, inquisitive, ambitious, and passionate astrophysicist.\n",
      "\n",
      "**Model's Response:** He was an astrophysicist with a passion for science.\n",
      "\n",
      "**Evaluation Criteria:**\n",
      "1. Does th...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def create_judge_prompt(entry):\n",
    "    \"\"\"\n",
    "    Create a prompt for the LLM judge to evaluate model response quality.\n",
    "    \n",
    "    Args:\n",
    "        entry: Dictionary containing instruction, input, output (expected), and model_response\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string for the judge\n",
    "    \"\"\"\n",
    "    instruction = entry['instruction']\n",
    "    user_input = entry['input']\n",
    "    expected_output = entry['output']\n",
    "    model_response = entry['model_response']\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert evaluator assessing the quality of AI model responses. \n",
    "Your task is to determine if the model's response correctly addresses the given instruction.\n",
    "\n",
    "**Instruction:** {instruction}\n",
    "\n",
    "**Input:** {user_input if user_input else \"N/A\"}\n",
    "\n",
    "**Expected Response:** {expected_output}\n",
    "\n",
    "**Model's Response:** {model_response}\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "1. Does the model's response correctly address the instruction?\n",
    "2. Is the response factually accurate and relevant?\n",
    "3. Does it align with the expected response in meaning and quality?\n",
    "\n",
    "**Instructions:**\n",
    "- Compare the model's response with the expected response\n",
    "- Consider if the model's response is semantically equivalent or of similar quality\n",
    "- Minor wording differences are acceptable if the meaning is correct\n",
    "- Output your evaluation in this EXACT format:\n",
    "\n",
    "VERDICT: [CORRECT/INCORRECT]\n",
    "EXPLANATION: [Brief explanation of your decision in 1-2 sentences]\n",
    "\n",
    "Respond now with your evaluation:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test the prompt creation\n",
    "test_entry = eval_data[0]\n",
    "test_prompt = create_judge_prompt(test_entry)\n",
    "print(\"Sample Judge Prompt:\")\n",
    "print(\"=\" * 80)\n",
    "print(test_prompt[:500] + \"...\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6355835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LLM Judge on first entry...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdict: âœ— INCORRECT\n",
      "Explanation: The model did not add four adjectives describing the sentence; it rewrites with a phrase â€œwith a passion for scienceâ€ rather than four descriptive adjectives, failing to meet the instruction.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluate_single_entry(entry, client, model=\"openai/gpt-5-nano\"):\n",
    "    \"\"\"\n",
    "    Evaluate a single entry using the LLM judge.\n",
    "    \n",
    "    Args:\n",
    "        entry: Dictionary containing instruction, input, output, and model_response\n",
    "        client: OpenAI client instance\n",
    "        model: Model to use for judging\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with verdict (bool), explanation (str), and raw_response (str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create judge prompt\n",
    "        prompt = create_judge_prompt(entry)\n",
    "        \n",
    "        # Call OpenAI API\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "                },\n",
    "            ],\n",
    "            temperature=1,  # Lower temperature for more consistent judgments\n",
    "        )\n",
    "        \n",
    "        # Get response\n",
    "        response = completion.choices[0].message.content\n",
    "        \n",
    "        # Parse verdict\n",
    "        verdict_match = re.search(r'VERDICT:\\s*(CORRECT|INCORRECT)', response, re.IGNORECASE)\n",
    "        explanation_match = re.search(r'EXPLANATION:\\s*(.+?)(?:\\n\\n|\\Z)', response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        is_correct = verdict_match and verdict_match.group(1).upper() == \"CORRECT\"\n",
    "        explanation = explanation_match.group(1).strip() if explanation_match else \"No explanation provided\"\n",
    "        \n",
    "        return {\n",
    "            \"is_correct\": is_correct,\n",
    "            \"explanation\": explanation,\n",
    "            \"raw_response\": response\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating entry: {e}\")\n",
    "        return {\n",
    "            \"is_correct\": False,\n",
    "            \"explanation\": f\"Error during evaluation: {str(e)}\",\n",
    "            \"raw_response\": \"\"\n",
    "        }\n",
    "\n",
    "# Test on a single entry\n",
    "print(\"Testing LLM Judge on first entry...\")\n",
    "print(\"=\" * 80)\n",
    "result = evaluate_single_entry(eval_data[0], client)\n",
    "print(f\"Verdict: {'âœ“ CORRECT' if result['is_correct'] else 'âœ— INCORRECT'}\")\n",
    "print(f\"Explanation: {result['explanation']}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feebd3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_judge_evaluation(eval_data, client, model=\"openai/gpt-5-nano\", max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate all entries using LLM-as-a-Judge approach.\n",
    "    \n",
    "    Args:\n",
    "        eval_data: List of evaluation entries\n",
    "        client: OpenAI client instance\n",
    "        model: Model to use for judging\n",
    "        max_samples: Maximum number of samples to evaluate (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - results: List of evaluation results for each entry\n",
    "            - accuracy: Overall accuracy percentage\n",
    "            - correct_count: Number of correct responses\n",
    "            - total_count: Total number of evaluated responses\n",
    "    \"\"\"\n",
    "    # Limit samples if specified\n",
    "    data_to_eval = eval_data[:max_samples] if max_samples else eval_data\n",
    "    \n",
    "    print(f\"Starting LLM-as-a-Judge evaluation on {len(data_to_eval)} samples...\")\n",
    "    print(f\"Using model: {model}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    correct_count = 0\n",
    "    \n",
    "    # Evaluate each entry\n",
    "    for i, entry in enumerate(tqdm(data_to_eval, desc=\"Evaluating\")):\n",
    "        result = evaluate_single_entry(entry, client, model)\n",
    "        \n",
    "        # Add entry info to result\n",
    "        result['index'] = i\n",
    "        result['instruction'] = entry['instruction']\n",
    "        result['expected_output'] = entry['output']\n",
    "        result['model_response'] = entry['model_response']\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        if result['is_correct']:\n",
    "            correct_count += 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    total_count = len(results)\n",
    "    accuracy = (correct_count / total_count * 100) if total_count > 0 else 0\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total Samples Evaluated: {total_count}\")\n",
    "    print(f\"Correct Responses: {correct_count}\")\n",
    "    print(f\"Incorrect Responses: {total_count - correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct_count\": correct_count,\n",
    "        \"total_count\": total_count\n",
    "    }\n",
    "\n",
    "# Example: Run evaluation on first 10 samples\n",
    "# Uncomment to run:\n",
    "# eval_results = llm_judge_evaluation(eval_data, client, max_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96600cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_samples(eval_results, num_samples=5, show_correct=True, show_incorrect=True):\n",
    "    \"\"\"\n",
    "    Display sample evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        eval_results: Dictionary returned by llm_judge_evaluation\n",
    "        num_samples: Number of samples to display\n",
    "        show_correct: Whether to show correct predictions\n",
    "        show_incorrect: Whether to show incorrect predictions\n",
    "    \"\"\"\n",
    "    results = eval_results['results']\n",
    "    \n",
    "    # Filter results\n",
    "    filtered_results = []\n",
    "    if show_correct:\n",
    "        filtered_results.extend([r for r in results if r['is_correct']])\n",
    "    if show_incorrect:\n",
    "        filtered_results.extend([r for r in results if not r['is_correct']])\n",
    "    \n",
    "    # Limit to num_samples\n",
    "    filtered_results = filtered_results[:num_samples]\n",
    "    \n",
    "    for i, result in enumerate(filtered_results):\n",
    "        status = \"âœ“ CORRECT\" if result['is_correct'] else \"âœ— INCORRECT\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Sample {result['index'] + 1} - {status}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Instruction: {result['instruction']}\")\n",
    "        print(f\"\\nExpected: {result['expected_output'][:200]}...\")\n",
    "        print(f\"\\nModel Response: {result['model_response'][:200]}...\")\n",
    "        print(f\"\\nJudge's Explanation: {result['explanation']}\")\n",
    "        print()\n",
    "\n",
    "# Example usage:\n",
    "# display_evaluation_samples(eval_results, num_samples=3, show_incorrect=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8850b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results(eval_results, filename=\"llm_judge_results.json\"):\n",
    "    \"\"\"\n",
    "    Save evaluation results to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        eval_results: Dictionary returned by llm_judge_evaluation\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(eval_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ“ Saved evaluation results to '{filename}'\")\n",
    "    print(f\"  - Total samples: {eval_results['total_count']}\")\n",
    "    print(f\"  - Accuracy: {eval_results['accuracy']:.2f}%\")\n",
    "\n",
    "def create_evaluation_report(eval_results):\n",
    "    \"\"\"\n",
    "    Create a detailed text report of the evaluation.\n",
    "    \n",
    "    Args:\n",
    "        eval_results: Dictionary returned by llm_judge_evaluation\n",
    "    \n",
    "    Returns:\n",
    "        String containing the formatted report\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"LLM-AS-A-JUDGE EVALUATION REPORT\")\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(f\"\\nTotal Samples Evaluated: {eval_results['total_count']}\")\n",
    "    report.append(f\"Correct Responses: {eval_results['correct_count']}\")\n",
    "    report.append(f\"Incorrect Responses: {eval_results['total_count'] - eval_results['correct_count']}\")\n",
    "    report.append(f\"Overall Accuracy: {eval_results['accuracy']:.2f}%\")\n",
    "    report.append(\"\\n\" + \"=\"*80)\n",
    "    report.append(\"INCORRECT PREDICTIONS ANALYSIS\")\n",
    "    report.append(\"=\"*80)\n",
    "    \n",
    "    incorrect_results = [r for r in eval_results['results'] if not r['is_correct']]\n",
    "    \n",
    "    for i, result in enumerate(incorrect_results[:10]):  # Show first 10 incorrect\n",
    "        report.append(f\"\\n{i+1}. Sample #{result['index'] + 1}\")\n",
    "        report.append(f\"   Instruction: {result['instruction'][:100]}...\")\n",
    "        report.append(f\"   Judge's Explanation: {result['explanation']}\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    if len(incorrect_results) > 10:\n",
    "        report.append(f\"\\n... and {len(incorrect_results) - 10} more incorrect predictions\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Example usage:\n",
    "# save_evaluation_results(eval_results, \"my_evaluation_results.json\")\n",
    "# print(create_evaluation_report(eval_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82773f2a",
   "metadata": {},
   "source": [
    "## How to Run the Complete Evaluation\n",
    "\n",
    "Now you have a complete LLM-as-a-Judge evaluation system! Here's how to use it:\n",
    "\n",
    "### Step 1: Quick Test (5-10 samples)\n",
    "Test on a small subset first to make sure everything works:\n",
    "```python\n",
    "eval_results = llm_judge_evaluation(eval_data, client, max_samples=10)\n",
    "```\n",
    "\n",
    "### Step 2: View Sample Results\n",
    "```python\n",
    "display_evaluation_samples(eval_results, num_samples=3, show_incorrect=True)\n",
    "```\n",
    "\n",
    "### Step 3: Full Evaluation\n",
    "Run on all samples (this may take a while):\n",
    "```python\n",
    "eval_results = llm_judge_evaluation(eval_data, client, max_samples=None)\n",
    "```\n",
    "\n",
    "### Step 4: Save and Report\n",
    "```python\n",
    "save_evaluation_results(eval_results, \"my_model_evaluation.json\")\n",
    "print(create_evaluation_report(eval_results))\n",
    "```\n",
    "\n",
    "### What the System Does:\n",
    "1. âœ… Takes each entry from your evaluation dataset\n",
    "2. âœ… Creates a structured prompt for the LLM judge\n",
    "3. âœ… Calls OpenAI API to evaluate if model response matches expected output\n",
    "4. âœ… Parses the judge's verdict (CORRECT/INCORRECT)\n",
    "5. âœ… Calculates overall accuracy percentage\n",
    "6. âœ… Provides detailed explanations for each evaluation\n",
    "7. âœ… Saves results to JSON for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed5f2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LLM-as-a-Judge evaluation on 50 samples...\n",
      "Using model: openai/gpt-5-nano\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [03:20<00:02,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating entry: Rate limit reached. Upgrade to the next tier for more requests: https://lightning.ai/pricing/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:24<00:00,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "Total Samples Evaluated: 50\n",
      "Correct Responses: 7\n",
      "Incorrect Responses: 43\n",
      "Accuracy: 14.00%\n",
      "================================================================================\n",
      "âœ“ Saved evaluation results to 'evaluation_results_sample.json'\n",
      "  - Total samples: 50\n",
      "  - Accuracy: 14.00%\n",
      "\n",
      "================================================================================\n",
      "LLM-AS-A-JUDGE EVALUATION REPORT\n",
      "================================================================================\n",
      "\n",
      "Total Samples Evaluated: 50\n",
      "Correct Responses: 7\n",
      "Incorrect Responses: 43\n",
      "Overall Accuracy: 14.00%\n",
      "\n",
      "================================================================================\n",
      "INCORRECT PREDICTIONS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. Sample #1\n",
      "   Instruction: Given a sentence, add in 4 adjectives that describe the sentence....\n",
      "   Judge's Explanation: The model did not add four adjectives describing the sentence; it rewrote it with a non-descriptive phrase and no four adjectives, thus not meeting the instruction.\n",
      "\n",
      "\n",
      "2. Sample #3\n",
      "   Instruction: Describe the following landscape with words...\n",
      "   Judge's Explanation: The response merely restates the elements and offers shallow, conflicting descriptions (a \"wide valley below\" and a \"small mountain in the center\"), failing to provide the vivid, cohesive description and imagery present in the expected answer.\n",
      "\n",
      "\n",
      "3. Sample #4\n",
      "   Instruction: Given a vector of numbers, calculate the mean of the vector....\n",
      "   Judge's Explanation: The computed mean is 9, but the model stated 12, which is wrong and does not match the expected result.\n",
      "\n",
      "\n",
      "4. Sample #5\n",
      "   Instruction: Determine the probability that a team will win the game given the equation P(x) = (x + 1) / (x + 3)....\n",
      "   Judge's Explanation: The model plugged in x=2 but did not compute the numeric probability (3/5 or 0.6); it merely restates the expression without evaluation and omits the accompanying interpretation.\n",
      "\n",
      "\n",
      "5. Sample #6\n",
      "   Instruction: Change the following adjective to its comparative form...\n",
      "   Judge's Explanation: The task requires providing the direct comparative form \"Better.\" The model outputs a sentence \"Good is better than bad,\" which does not supply the requested isolated comparative adjective form.\n",
      "\n",
      "\n",
      "6. Sample #7\n",
      "   Instruction: Categorize the given sentence according to grammar...\n",
      "   Judge's Explanation: The model labeled the sentence as \"Adjective,\" which is not a grammatical category of the sentence. The correct categorization is that it is a declarative sentence in the present perfect tense.\n",
      "\n",
      "\n",
      "7. Sample #9\n",
      "   Instruction: Generate a metaphor about the power of failure....\n",
      "   Judge's Explanation: The model provides a metaphor that frames failure as a dangerous, unstoppable boulder, which does not convey the empowering, growth-oriented idea of failureâ€™s power as suggested in the expected metaphor about sowing seeds and blooms.\n",
      "\n",
      "\n",
      "8. Sample #10\n",
      "   Instruction: Write a poem about the Earth and its beauty....\n",
      "   Judge's Explanation: The response is repetitive and lacks substantive imagery about Earth's beauty. It does not resemble the expected poem's imagery or breadth, and fails to provide a meaningful, varied depiction of the Earth.\n",
      "\n",
      "\n",
      "9. Sample #11\n",
      "   Instruction: Explain how to build a professional portfolio....\n",
      "   Judge's Explanation: The model fails to describe practical steps for building a professional portfolio (platform selection, organizing content, including items, design, sharing). Instead, it repeats vague, unrelated guidance about researching skills, which does not align with the expected process.\n",
      "\n",
      "\n",
      "10. Sample #12\n",
      "   Instruction: Come up with a 100-word short story about a parent who discovers their son is being cyberbullied....\n",
      "   Judge's Explanation: The model did not produce a 100-word short story and misses the required focus on a parent discovering their son is being cyberbullied, instead giving a brief, repetitive narrative about seeking online help. It lacks the depth and specifics of the expected example.\n",
      "\n",
      "\n",
      "... and 33 more incorrect predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN EVALUATION HERE\n",
    "# Uncomment the lines below to run the evaluation\n",
    "\n",
    "# Step 1: Test on a small sample first (recommended)\n",
    "eval_results = llm_judge_evaluation(eval_data, client, max_samples=50)\n",
    "\n",
    "# Step 2: View some results\n",
    "#display_evaluation_samples(eval_results, num_samples=3, show_incorrect=True)\n",
    "\n",
    "# Step 3: Save results\n",
    "save_evaluation_results(eval_results, \"evaluation_results_sample.json\")\n",
    "\n",
    "# Step 4: Print report\n",
    "print(\"\\n\" + create_evaluation_report(eval_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL EVALUATION (Run this after testing with small samples)\n",
    "# This will evaluate ALL 500 samples - may take several minutes\n",
    "\n",
    "# Uncomment to run full evaluation:\n",
    "# print(\"Starting full evaluation on all 500 samples...\")\n",
    "# print(\"This may take 5-10 minutes depending on API speed...\")\n",
    "# \n",
    "# full_eval_results = llm_judge_evaluation(eval_data, client, max_samples=None)\n",
    "# \n",
    "# # Save full results\n",
    "# save_evaluation_results(full_eval_results, \"full_evaluation_results.json\")\n",
    "# \n",
    "# # Generate and save report\n",
    "# report = create_evaluation_report(full_eval_results)\n",
    "# print(\"\\n\" + report)\n",
    "# \n",
    "# with open(\"evaluation_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(report)\n",
    "# print(\"\\nâœ“ Report saved to 'evaluation_report.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90deaae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Total Samples: 100\n",
      "âœ… Correct: 9 (9.0%)\n",
      "âŒ Incorrect: 91 (91.0%)\n",
      "\n",
      "ðŸŽ¯ Overall Accuracy: 9.00%\n",
      "\n",
      "================================================================================\n",
      "Visual Breakdown:\n",
      "================================================================================\n",
      "Correct:   [â–ˆâ–ˆâ–ˆâ–ˆ                                              ] 9\n",
      "Incorrect: [    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 91\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualization: Show evaluation statistics\n",
    "def plot_evaluation_stats(eval_results):\n",
    "    \"\"\"\n",
    "    Display evaluation statistics in a formatted way.\n",
    "    \"\"\"\n",
    "    correct = eval_results['correct_count']\n",
    "    incorrect = eval_results['total_count'] - correct\n",
    "    accuracy = eval_results['accuracy']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nðŸ“Š Total Samples: {eval_results['total_count']}\")\n",
    "    print(f\"âœ… Correct: {correct} ({correct/eval_results['total_count']*100:.1f}%)\")\n",
    "    print(f\"âŒ Incorrect: {incorrect} ({incorrect/eval_results['total_count']*100:.1f}%)\")\n",
    "    print(f\"\\nðŸŽ¯ Overall Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # ASCII bar chart\n",
    "    bar_length = 50\n",
    "    correct_bar = int(bar_length * correct / eval_results['total_count'])\n",
    "    incorrect_bar = bar_length - correct_bar\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Visual Breakdown:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Correct:   [{'â–ˆ' * correct_bar}{' ' * incorrect_bar}] {correct}\")\n",
    "    print(f\"Incorrect: [{' ' * correct_bar}{'â–ˆ' * incorrect_bar}] {incorrect}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Example usage:\n",
    "plot_evaluation_stats(eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f99d8e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
